{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0a761b4",
   "metadata": {},
   "source": [
    "# Classification with Random Forest Classifier  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ac3dd",
   "metadata": {},
   "source": [
    "After cleaning and analysing the dataset we proceed with a classifier, which is carried on with the random forest classifier algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c2863-a9b0-4c4b-b23d-114435e6899f",
   "metadata": {},
   "source": [
    "![random-forest.jpg](random-forest.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb938e7",
   "metadata": {},
   "source": [
    "## Preparing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc9142",
   "metadata": {},
   "source": [
    "We import the relevant libraries from ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_curve, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd05833-1627-4788-b457-34b904984b09",
   "metadata": {},
   "source": [
    "We import the data from `TWER_grouped_class.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0d89b5-614a-4ea7-8e9f-af8a390c29ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/TWER_grouped_class.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e5267a-7b32-4402-a2a5-44b2150d08f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17a18e7-641c-47d1-b3a1-a5cd601c1b23",
   "metadata": {},
   "source": [
    "Before continuing, we need to remove the last day (2013-12-31), on which we will make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c958da79-20ec-4f80-bc5c-8fb1ee69fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['day'] != 31]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4a58b9-decf-4bf0-9b61-403b01eb422a",
   "metadata": {},
   "source": [
    "Let's divide `df` into features and target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b814363c-dbb1-494d-9b89-07556e98b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1)\n",
    "y = df['class']  # target is the multi-class label (High, Medium, Low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798f191-835e-4790-8a44-bbe3bccfe82b",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fdd5d1-d7ae-4307-a6ca-c80c2f97da1f",
   "metadata": {},
   "source": [
    "We now preprocess the data. \n",
    "The classifier only really accepts numbers as an input, so string-to-number conversion of categorical data is essential.\n",
    "Since this process only concerns categorical data,  we need to split numerical and categorical features first. Second, we enconde the categorical data with label encoding. \n",
    "\n",
    "Then we prepare for the classification by splitting Train Data and Test Data. We want the test data to be the 20% of the data available (test_size=0.2) and we want to fix a random_state value of 20.\n",
    "\n",
    "This is basically like a seed and assures the \"random behaviour\" of the forest, to be always the same if we run the program the program multiple times.\n",
    "\n",
    "In the fase of tuning repeatability is essential, because it is the only way to reliably tune the parameters of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6251c",
   "metadata": {},
   "source": [
    "We then scale the numerical features, convert them back to dataframe form and finally recombine categorical and numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f3857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, y, categorical_features, numerical_features, test_size=0.2, random_state=20):\n",
    "    # Label encode the target\n",
    "    le_target = LabelEncoder()\n",
    "    y = le_target.fit_transform(y)\n",
    "\n",
    "    # Label encode categorical features\n",
    "    le_feature = LabelEncoder()\n",
    "    for feat in categorical_features:\n",
    "        X[feat] = le_feature.fit_transform(X[feat])\n",
    "\n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "    X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6322e803",
   "metadata": {},
   "source": [
    "\n",
    "We then initialise and train the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454128d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, X_test, y_train, y_test, n_estimators, max_depth=None):\n",
    "    # Initialize Random Forest Classifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=20)\n",
    "    \n",
    "    # Train the classifier\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Accuracy with n_estimators={n_estimators} and max_depth={max_depth}: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g', ax=ax)\n",
    "    ax.xaxis.set_ticklabels(['high', 'low', 'medium'])\n",
    "    ax.yaxis.set_ticklabels(['high', 'low', 'medium'])\n",
    "    plt.show()\n",
    "    \n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29412ed9",
   "metadata": {},
   "source": [
    "In the developement of our analysis we noticed that it was not obvious whether to use all the data we had available or just a part of that. \n",
    "\n",
    "We intend to discuss this matter by examining two data sets: one that basically trains exlusively on tweet counts and the ARPA's weather data and the other on the largest possible dataset, which also has electrical data and other potentially meaningful features such as `population`, `elevation`. Naturally these data appear with their relative time slot and municipality. \n",
    "\n",
    "We do the traing for both dataset with respectively a little forest `(n_estimators=4, max_depth=4)` and a large forest `(n_estimators=100, no max_depth)`. \n",
    "In order to do that we created a function that will execute, with any given parameter, all the steps of our classification. \n",
    "\n",
    "Indeed the function:\n",
    "1) Splits the data set in training and test data \n",
    "2) Trains the Classifier \n",
    "3) Commutes the prediction\n",
    "4) Prints all the meaningful extimators\n",
    "5) Plots the confusion matrix and the heatmap "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b25ed9",
   "metadata": {},
   "source": [
    "We create the two relevant Dataframes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1 (weather data)\n",
    "X_weather = df.drop('class', axis=1).drop(['curr_cell', 'curr_site', 'population', 'elevation'], axis=1)\n",
    "y_weather = df['class']\n",
    "\n",
    "# Categorical and numerical features\n",
    "categorical_features_weather = ['date', 'municipality.name', 'hour_category', 'month', 'day_of_week']\n",
    "numerical_features_weather = ['temperature', 'minTemperature', 'maxTemperature', 'precipitation', 'wind_speed', 'wind_dir', 'tweet_count', 'day']\n",
    "\n",
    "# Preprocess Dataset 1\n",
    "X_weather_train, X_weather_test, y_weather_train, y_weather_test = preprocess_data(X_weather, y_weather, categorical_features_weather, numerical_features_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 2 (weather + other)\n",
    "X_additional = df.drop('class', axis=1)\n",
    "y_additional = df['class']\n",
    "\n",
    "# Categorical and numerical features\n",
    "categorical_features_additional = ['date', 'municipality.name', 'hour_category', 'month', 'day_of_week']\n",
    "numerical_features_additional = ['temperature', 'minTemperature', 'maxTemperature', 'precipitation', 'wind_speed', 'wind_dir', \n",
    "                                 'curr_cell', 'population', 'elevation', 'curr_site', 'tweet_count', 'day']\n",
    "\n",
    "# Preprocess Dataset 2\n",
    "X_additional_train, X_additional_test, y_additional_train, y_additional_test = preprocess_data(X_additional, y_additional, categorical_features_additional, numerical_features_additional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886275a0-2ad1-458e-bda5-1b829aa2e3fd",
   "metadata": {},
   "source": [
    "## RF classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ed5dae",
   "metadata": {},
   "source": [
    "We herby presents all the different scenarios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eaf990",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scenario 1: Weather Data with n_estimators=4 and max_depth=4\")\n",
    "conf_matrix1 = train_and_evaluate(X_weather_train, X_weather_test, y_weather_train, y_weather_test, n_estimators=4, max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ecae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scenario 2: Weather + Additional Data with n_estimators=4 and max_depth=4\")\n",
    "conf_matrix2 = train_and_evaluate(X_additional_train, \n",
    "                                  X_additional_test, \n",
    "                                  y_additional_train, \n",
    "                                  y_additional_test, \n",
    "                                  n_estimators=4, \n",
    "                                  max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c17c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scenario 3: Weather Data with n_estimators=100 and no max_depth\")\n",
    "conf_matrix3 = train_and_evaluate(X_weather_train, \n",
    "                                  X_weather_test,\n",
    "                                  y_weather_train, \n",
    "                                  y_weather_test,\n",
    "                                  n_estimators=100, \n",
    "                                  max_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae66fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scenario 4: Weather + Additional Data with n_estimators=100 and no max_depth\")\n",
    "conf_matrix4 = train_and_evaluate(X_additional_train,\n",
    "                                  X_additional_test,\n",
    "                                  y_additional_train,\n",
    "                                  y_additional_test,\n",
    "                                  n_estimators=100, \n",
    "                                  max_depth=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cae823",
   "metadata": {},
   "source": [
    "This result is not obvious, the so called additional data are not correlated as well to tweet count as the whether data, the risk of redundancy or even insignificance of the dataset is high. Indeed from a first trial it seemed to us the results went in this direction. The risk of overfitting for a larger dataset is high if there are redundancies, since the model may overlearn patterns. If the \"additional data\" introduces noise instead of information, it's better to remove that part of the dataset entirely. \n",
    "\n",
    "Nevertheless, we notice a neat improvement by introducing new data and this signals that the additional data is indeed informative. \n",
    "\n",
    "Beyond that, what we notice is that little forests perform way worse than larger forests, a more in depth  training and a meaningful number of estimators is crucial to generalisation. \n",
    "This does not depend on the dataset but it is very general, the eventual point in setting a low number of estimators and max_depth parameter is to make the training computationally feasible, but for us it's no problem since the whole process is very fast. If we were to analyse the telecommunication data, the correct approach may very well be using low max_depth and low n_estimators. \n",
    "\n",
    "As indicated by the the accuracy, the classifier always works fine, even in the little forest case. \n",
    "In the last case (larger forest on larger dataset), we see the promising result of just two misclassifications.\n",
    "\n",
    "We point out that, as emerges form the confusion matrix, the error type is (almost) always the same in the four cases: some municipalities, which are predicted to be in the middle tweet count range, actually end up to be in the high tweet count range.\n",
    "\n",
    "This makes sense, because, whereas there are not many events outside our dataset which can cause great short term depression in the tweet count, there are lots of social events that may cause an occasional peak in the tweet count. In the dates Dec 13th and 14th 2013, which present the greatest tweet predictive mistake, a big event called **Universiadi** was held in the municipality of Trento, where such peak is registered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57f7bcb-4300-4dd3-b4d2-47342ac9d838",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7711f5c7-437b-4b9b-b058-2993f5247427",
   "metadata": {},
   "source": [
    "Now that we estalished the validity of our model, we can train it on all data before December 31st, the day of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e3f6f-146a-460a-a76a-93017fc00a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/TWER_grouped_class.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884cd3fe-5091-4606-aefc-3a1cf524b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae38efa-028a-4050-869c-9dca5aaf7392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf9e857-8a96-48e0-80cc-c2f72dfb3817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_features = ['date', 'municipality.name', 'hour_category', 'month', 'day_of_week']  # Any string or categorical features\n",
    "numerical_features = ['temperature', \n",
    "                      'minTemperature', \n",
    "                      'maxTemperature', \n",
    "                      'precipitation', \n",
    "                      'wind_speed',\n",
    "                      'wind_dir',\n",
    "                      'curr_cell',\n",
    "                      'population',\n",
    "                      'elevation',\n",
    "                      'tweet_count',\n",
    "                      'curr_site',\n",
    "                      'day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a920c-6ac5-4942-83fd-d7bb44ab7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Dec 31st\n",
    "len31 = len(X[X['date'] == '2013-12-31'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648cb3ad-ff51-4f46-87e7-dfa6979aedb3",
   "metadata": {},
   "source": [
    "Now the train-test split is done by selecting only the days before Dec 31st."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af709f-42f6-4411-9277-acb104169a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for categorical features and target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)  # Apply LabelEncoder to the entire target variable\n",
    "\n",
    "# Encode the categorical features for the entire dataset before splitting\n",
    "for feat in categorical_features:\n",
    "    X[feat] = le.fit_transform(X[feat])\n",
    "\n",
    "# Split the dataset after label encoding\n",
    "X_train, X_test = X[0:len(X)-len31], X[len(X)-len31:]\n",
    "y_train, y_test = y[0:len(X)-len31], y[len(X)-len31:]\n",
    "\n",
    "# Scale numerical features after splitting\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "# Training the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=20)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting and evaluating\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', ax=ax)\n",
    "ax.xaxis.set_ticklabels(['high', 'low', 'medium'])\n",
    "ax.yaxis.set_ticklabels(['high', 'low', 'medium'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c897e-36f5-41a6-ae86-f59d6c768471",
   "metadata": {},
   "source": [
    "That's what we wanted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
