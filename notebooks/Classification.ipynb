{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0a761b4",
   "metadata": {},
   "source": [
    "# Classification with Random Forest Classifier  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ac3dd",
   "metadata": {},
   "source": [
    "After cleaning and analysing the dataset we proceed with a regression, which is carried on with the random forest classifier algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb938e7",
   "metadata": {},
   "source": [
    "## Preparing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01fa2d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"[random-forest.jpg]\" non Ã¨ riconosciuto come comando interno o esterno,\n",
      " un programma eseguibile o un file batch.\n"
     ]
    }
   ],
   "source": [
    "![random-forest.jpg](attachment:ca783f13-594d-4e23-bc3b-8428378dffe3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc9142",
   "metadata": {},
   "source": [
    "We import the relevant libraries from ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c2e6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_curve, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6251c",
   "metadata": {},
   "source": [
    "We now preprocess the data. \n",
    "The classifier only really accepts numbers as an input, so string-to-number conversion of categorical data is essential.\n",
    "Since this process only concerns categorical data,  we need to split numerical and categorical features first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674b716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categorical_features = ['date', 'municipality.name', 'hour_category']  # Any string or categorical features\n",
    "numerical_features = ['temperature', \n",
    "                      'minTemperature', \n",
    "                      'maxTemperature', \n",
    "                      'precipitation', \n",
    "                      'tweet_count', \n",
    "                      'wind_speed',\n",
    "                      'wind_dir',\n",
    "                      'curr_cell',\n",
    "                      'curr_site']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7939e741",
   "metadata": {},
   "source": [
    "Second, we enconde the categorical data with label encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c6fc04c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabelEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Encode categorical features\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m le \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feat \u001b[38;5;129;01min\u001b[39;00m categorical_features:\n\u001b[0;32m      4\u001b[0m     X[feat] \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mfit_transform(X[feat])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LabelEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "# Encode categorical features\n",
    "le = LabelEncoder()\n",
    "for feat in categorical_features:\n",
    "    X[feat] = le.fit_transform(X[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[categorical_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb063f1",
   "metadata": {},
   "source": [
    "We then scale the numerical features, convert them back to dataframe form and finally recombine categorical and numerical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676411a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "scaled_numerical = scaler.fit_transform(X[numerical_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2864e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "scaled_numerical = scaler.fit_transform(X[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78ea4dab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Combine the scaled numerical features and encoded categorical features\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([scaled_numerical_df, X[categorical_features]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Combine the scaled numerical features and encoded categorical features\n",
    "X = pd.concat([scaled_numerical_df, X[categorical_features].astype('category')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927abcda",
   "metadata": {},
   "source": [
    "This is now a dataframe we can work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92926ca1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X\u001b[38;5;241m.\u001b[39mdtypes\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6322e803",
   "metadata": {},
   "source": [
    "Now we prepare for the regression by splitting Train Data and Test Data. \n",
    "We want the test data to be the 20 % of the data available (test_size=0.2) and we want to fix a random_state value of 20. \n",
    "\n",
    "This is basically like a seed and assures the \"random behaviour of the forest, to be always the same if we run the program the program multiple times. \n",
    "\n",
    "In the fase of tuning repeatability is essential, because it is the only way to reliably tune the parameters of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aff4dcf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e1ad8",
   "metadata": {},
   "source": [
    "We initialise and train the Random Forest Classifier. \n",
    "We initialise it to a pretty canonical number of estimators (100) and to the already mentioned random_state=20.  \n",
    "We don't use a max_widht parameter since the completion of the random forest's training is very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "486b747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb356193",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m rf_classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c6662a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3451178437.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[31], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Eventually, we obtain a prediction on the data\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Eventually, we obtain a prediction on the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98becec3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m rf_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b8ecd7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (945106861.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[34], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    We calculate here some meaningful extimators\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "We calculate here some meaningful extimators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4249df64",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_score(y_test,\u001b[38;5;250m \u001b[39my_pred)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8547ce01",
   "metadata": {},
   "source": [
    "We notice the accuracy is really close to 1, which indicates the goodnes of the described model. \n",
    "It is then a good idea to plot a confusion matrix and a heatmap to see what points have been misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "040ddf17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_score(y_test,\u001b[38;5;250m \u001b[39my_pred)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g', ax=ax)\n",
    "ax.xaxis.set_ticklabels(['high', 'low', 'medium'])\n",
    "ax.yaxis.set_ticklabels(['high', 'low', 'medium'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851907b9",
   "metadata": {},
   "source": [
    "The remarkable fact is that just 1 point has been misclassified . \n",
    "\n",
    "This prediction is possible with a particular choice of the dataset. \n",
    "\n",
    "The only set of features that appears to be relevant is that related to the weather, and indeed the changing of the classifier's random_state variable does not alter this fact.\n",
    "\n",
    "The strongest predictor for tweet count is arguably temperature, that is evident by seeing at the plot which is available in the EDA notebook.\n",
    "\n",
    "We noticed that some features as month, day, day of the week and electrical data, actually worsen the model. More values are in fact mispredicted.\n",
    "The error is always the same, some municipalities, which are predicted to be in the middle tweet count range, actually end up to be in the high tweet count range. \n",
    "\n",
    "This makes sense, because, whereas there are not many events outside our dataset which can cause great short term depression in the tweet count, there are lots of social events that may cause an occasional peak in the tweet count. \n",
    "In the dates 13 and 14 Decembers 2013, which present the greatest tweet predictive mistake, a big event called \"Universiadi \" was held in the municipality of Trento, where such peak is registered. \n",
    "\n",
    "If we insert those data, we have a bigger variance because we enlarge the model with data which is not significantly correlated.\n",
    "We may be experimenting an overfitting phenomenon, increasing the features of the data requires increasing information to effectively model patterns without overfitting.\n",
    "If the mentioned data are not higly informative, and our EDA results suggest precisely that, we can very well incur in this overfitting problem.  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
